{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#find #look for word2vec if words similar to these comes then use regex to fetch the next file name\n",
        "#dataset comparision to the words\n",
        "#if the similarity is mmore than 60-70% then extract the words next to it usually there will be the file name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRZxBba1Cwhw",
        "outputId": "d6b67859-3053-43cf-82bc-44764447ce48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted File Name: 19_12_2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiFpGIwsFjM1",
        "outputId": "524ac54e-b668-44b0-8236-3a7f5116ed7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a0iBIQcJxup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f55db87f-74bb-4cc7-8611-1f47458a0c79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[=================================================-] 99.9% 1661.9/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "'''import gensim.downloader as api\n",
        "\n",
        "wv = api.load('word2vec-google-news-300')'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import pandas as pd\n"
      ],
      "metadata": {
        "id": "MP0pepgFu5PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df= pd.read_csv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2YB4TQ3u9j-",
        "outputId": "0c574782-8e5f-4a6c-d6cf-3211833d542c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matched expression: gerber, Date: 19/12/2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''import gensim.downloader as api\n",
        "import spacy\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def load_word2vec_model():\n",
        "    # Load the pre-trained Word2Vec model\n",
        "    model_name = \"word2vec-google-news-300\"\n",
        "    model = api.load(model_name)\n",
        "    return model\n",
        "\n",
        "def get_word_vector(word, model):\n",
        "    # Get the vector representation of a word from the Word2Vec model\n",
        "    if word in model:\n",
        "        return model[word]\n",
        "    return None\n",
        "\n",
        "def get_most_similar_words(word, model, top_n=5):\n",
        "    # Get the top_n most similar words to the given word based on cosine similarity\n",
        "    similar_words = model.most_similar(positive=[word], topn=top_n)\n",
        "    return [word[0] for word in similar_words]\n",
        "\n",
        "def extract_file_name_using_similarity(email_message):\n",
        "    # Load the spaCy English model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Process the email message using spaCy\n",
        "    doc = nlp(email_message)\n",
        "\n",
        "    # Load the pre-trained Word2Vec model\n",
        "    model = load_word2vec_model()\n",
        "\n",
        "    # Initialize a variable to store the file name\n",
        "    file_name = None\n",
        "\n",
        "    # Look for words with similar meanings to keywords\n",
        "    keywords = [\"please\", \"find\", \"attached\", \"file\"]\n",
        "    similar_keywords = [get_most_similar_words(keyword, model) for keyword in keywords]\n",
        "\n",
        "    # Iterate through the document tokens\n",
        "    for i, token in enumerate(doc):\n",
        "        if token.text.lower() in keywords:\n",
        "            # Check the next few tokens for words with similar meanings\n",
        "            for j in range(i + 1, i + 6):\n",
        "                if doc[j].text.lower() in similar_keywords:\n",
        "                    # Extract the file name from the next token\n",
        "                    file_name = doc[j + 1].text\n",
        "                    break\n",
        "            if file_name:\n",
        "                break\n",
        "\n",
        "    # Return the extracted file name if found, otherwise return None\n",
        "    return file_name\n",
        "\n",
        "# Example input:\n",
        "message = \"\"\"Hello,\n",
        "Please find Gerber (19_12_2021) fabrication, please send the quote. Please ensure VIPPO is taken care properly by CAD manufacturing team.\n",
        "Quantity = 10 Units. We need it at the earliest. Thanks.\n",
        "Best Regards\n",
        "Satyabrata Mohanty\"\"\"\n",
        "\n",
        "file_name = extract_file_name_using_similarity(message)\n",
        "print(\"Extracted File Name:\", file_name)\n",
        "'''"
      ],
      "metadata": {
        "id": "qY4md3iF5n3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81447090-d231-4ce9-ce60-6947b251437c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted File Name: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q7dVPjpCfV6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "LEiiXWc1BkRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "message = \"\"\"Hello,\n",
        "Please find Gerber (19_12_2021) fabrication, please send the quote. Please ensure VIPPO is taken care properly by CAD manufacturing team.\n",
        "Quantity = 10 Units. We need it at the earliest. Thanks.\n",
        "Best Regards\n",
        "Satyabrata Mohanty\"\"\"\n",
        "\n",
        "sentences = nltk.sent_tokenize(message)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6LcvC3o_3rD",
        "outputId": "a63e9924-0dd2-4d81-e835-a0837062e0af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello,\\nPlease find Gerber (19_12_2021) fabrication, please send the quote.',\n",
              " 'Please ensure VIPPO is taken care properly by CAD manufacturing team.',\n",
              " 'Quantity = 10 Units.',\n",
              " 'We need it at the earliest.',\n",
              " 'Thanks.',\n",
              " 'Best Regards\\nSatyabrata Mohanty']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "message = \"\"\"Hello,\n",
        "Please find Gerber (19_12_2021) fabrication, please send the quote. Please ensure VIPPO is taken care properly by CAD manufacturing team.\n",
        "Quantity = 10 Units. We need it at the earliest. Thanks.\n",
        "Best Regards\n",
        "Satyabrata Mohanty\"\"\"\n",
        "tokens = sent_tokenize(message)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "6Q08KZG9AdNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "198d2dc7-996a-499f-e5a1-ab537606f3ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,\\nPlease find Gerber (19_12_2021) fabrication, please send the quote.', 'Please ensure VIPPO is taken care properly by CAD manufacturing team.', 'Quantity = 10 Units.', 'We need it at the earliest.', 'Thanks.', 'Best Regards\\nSatyabrata Mohanty']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def split_paragraph_into_sentences(paragraph):\n",
        "    sentences = re.split(r'(?<=[.,!;:])\\s+', paragraph)\n",
        "    return sentences\n",
        "\n",
        "paragraph = \"Hello, Please find Gerber (19_12_2021) fabrication, please send the quote. Please ensure VIPPO is taken care properly by CAD manufacturing team. Quantity = 10 Units. We need it at the earliest. Thanks. Best Regards Satyabrata Mohanty\"\n",
        "\n",
        "sentences = split_paragraph_into_sentences(paragraph)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"{i+1}. {sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJB7AR3jRkGF",
        "outputId": "c2066a1d-7053-42c7-c991-8cb1b0196dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Hello,\n",
            "2. Please find Gerber (19_12_2021) fabrication,\n",
            "3. please send the quote.\n",
            "4. Please ensure VIPPO is taken care properly by CAD manufacturing team.\n",
            "5. Quantity = 10 Units.\n",
            "6. We need it at the earliest.\n",
            "7. Thanks.\n",
            "8. Best Regards Satyabrata Mohanty\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "full_string = \"\"\"Hello,\n",
        "Please find Gerber (19_12_2021) fabrication, please send the quote. Please ensure VIPPO is taken care properly by CAD manufacturing team.\n",
        "Quantity = 10 Units. We need it at the earliest. Thanks.\n",
        "Best Regards\n",
        "Satyabrata Mohanty\"\"\"\n",
        "substring = \"please find\"\n",
        "\n",
        "# Create a regular expression pattern with the substring and the IGNORECASE flag\n",
        "pattern = re.compile(substring, re.IGNORECASE)\n",
        "\n",
        "# Search for the pattern in the full string\n",
        "match = pattern.search(full_string)\n",
        "\n",
        "if match:\n",
        "    end_index = match.end()\n",
        "    punctuation_positions = [pos for pos, char in enumerate(full_string[end_index:]) if char in '.?!,;']\n",
        "    if punctuation_positions:\n",
        "        nearest_punctuation_index = end_index + min(punctuation_positions)\n",
        "    else:\n",
        "        nearest_punctuation_index = len(full_string)\n",
        "\n",
        "    # Extract the remaining part of the string after the substring up to the nearest punctuation\n",
        "    remaining_substring = full_string[end_index:nearest_punctuation_index].strip()\n",
        "\n",
        "    print(\"Remaining substring:\", remaining_substring)\n",
        "else:\n",
        "    print(\"Substring not found (case-insensitive)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr-g5Wb7fX-4",
        "outputId": "6e951ced-2e7d-46a7-b42c-5b67cbba8082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remaining substring: Gerber (19_12_2021) fabrication\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-fIBJu5rfdA2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}